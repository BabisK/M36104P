{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charalampos Kaidos & Ioannis Papantonis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same directory as this notebook, create a folder named \"mnist\" and put inside the mnist data that where shipped with the project anouncement. In a separate folder called \"cifar-10-batches-py\" put the cifar10 pickled dataset as provided for python in the CIFAR website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow load MNIST data set from the files provided in the excercise. Also there are functions to save the data in pickled format and load them from pickled format which is much faster than loading them from the texr files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "def load_mnist(path):\n",
    "    print('Reading MNIST dataset from storage')\n",
    "\n",
    "    trainfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and 'train' in f]\n",
    "    train_data, train_target = load_mnist_from_files(trainfiles)\n",
    "\n",
    "    testfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and 'test' in f]\n",
    "    test_data, test_target = load_mnist_from_files(testfiles)\n",
    "\n",
    "    print('Finished reading MNIST')\n",
    "    return train_data, train_target, test_data, test_target\n",
    "\n",
    "def load_mnist_from_files(files):\n",
    "    data = None\n",
    "    target = None\n",
    "\n",
    "    for file in files:\n",
    "        print('Reading file {}'.format(file))\n",
    "        temp_data = numpy.loadtxt(file)\n",
    "        if (numpy.any(data) == None):\n",
    "            data = temp_data\n",
    "        else:\n",
    "            data = numpy.append(data, temp_data, axis=0)\n",
    "\n",
    "        rows, columns = temp_data.shape\n",
    "        temp_target = numpy.zeros((rows, 10))\n",
    "        temp_target[:, int(file[-5])] = numpy.ones(rows)\n",
    "        if (numpy.any(target) == None):\n",
    "            target = temp_target\n",
    "        else:\n",
    "            target = numpy.append(target, temp_target, axis=0)\n",
    "\n",
    "        print('Read {} rows, {} columns from {}'.format(rows, columns, file))\n",
    "\n",
    "    return data, target\n",
    "\n",
    "def pickle_mnist(traindata, traintarget, testdata, testtarget):\n",
    "    f=open('./mnist/trdata', mode='wb+')\n",
    "    pickle.dump(traindata, f)\n",
    "    f.close()\n",
    "    f = open('./mnist/trtarget', mode='wb+')\n",
    "    pickle.dump(traintarget, f)\n",
    "    f.close()\n",
    "    f = open('./mnist/tedata', mode='wb+')\n",
    "    pickle.dump(testdata, f)\n",
    "    f.close\n",
    "    f = open('./mnist/tetarget', mode='wb+')\n",
    "    pickle.dump(testtarget, f)\n",
    "    f.close()\n",
    "\n",
    "def unpickle_mnist():\n",
    "    fo = open('./mnist/trdata', 'rb')\n",
    "    traindata = pickle.load(fo)\n",
    "    fo.close()\n",
    "    fo = open('./mnist/trtarget', 'rb')\n",
    "    traintarget = pickle.load(fo)\n",
    "    fo.close()\n",
    "    fo = open('./mnist/tedata', 'rb')\n",
    "    testdata = pickle.load(fo)\n",
    "    fo.close()\n",
    "    fo = open('./mnist/tetarget', 'rb')\n",
    "    testtarget = pickle.load(fo)\n",
    "    fo.close()\n",
    "    return traindata, traintarget, testdata, testtarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow load the CIFAR-10 data set from the pickled format provided on the CIFAR website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "def load_cifar(path):\n",
    "    print('Reading CIFAR-10 dataset from storage')\n",
    "\n",
    "    dicts = []\n",
    "\n",
    "    trainfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and 'data_batch' in f]\n",
    "    for f in trainfiles:\n",
    "        dicts.append(unpickle(f))\n",
    "\n",
    "    data = numpy.vstack(tuple([x['data'] for x in dicts]))\n",
    "    ones = numpy.ones((data.shape[0], 1))\n",
    "    data = numpy.hstack((ones,data))\n",
    "\n",
    "    temp_target = numpy.hstack(tuple([x['labels'] for x in dicts]))\n",
    "    target = numpy.zeros((temp_target.shape[0], 10))\n",
    "    for r in range(temp_target.shape[0]):\n",
    "        target[r, temp_target[r]] = 1\n",
    "\n",
    "    test_data = unpickle(join(path, 'test_batch'))\n",
    "    test_target = numpy.zeros((len(test_data['labels']), 10))\n",
    "    for r in range(test_target.shape[0]):\n",
    "        test_target[r, test_data['labels'][r]] = 1\n",
    "    test_data = test_data['data']\n",
    "    ones = numpy.ones((test_data.shape[0], 1))\n",
    "    test_data = numpy.hstack((ones, test_data))\n",
    "\n",
    "    return data, target, test_data, test_target\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dictionary = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow implement the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def likelihood_function(W, T, Y, lam):\n",
    "    '''Calculate the cost for the given W'''\n",
    "    return numpy.sum(T * numpy.log(Y + numpy.finfo(float).eps)) - lam / 2 * numpy.sum(numpy.linalg.norm(W, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def softmax(W, X):\n",
    "    ''' Calculate softmax function for A=XW'''\n",
    "    Y = numpy.dot(X, W.T)\n",
    "    Y = Y - numpy.amax(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "    Y = numpy.exp(Y)\n",
    "    Y = Y / numpy.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "    return Y\n",
    "\n",
    "\n",
    "def gradient_descent(X, T, step, iterations, precision):\n",
    "    ''' Gradient descent to find optimal weights given a training set X,T\n",
    "        We use momentum to accelerate the convergence'''\n",
    "\n",
    "    def init_weights():\n",
    "        numpy.random.seed(5)\n",
    "        return deque((numpy.random.randn(T.shape[1], X.shape[1]), None, None))\n",
    "\n",
    "    def init_momentum():\n",
    "        return deque((0, None, None))\n",
    "\n",
    "    def init_cost():\n",
    "        return deque((-numpy.inf, -numpy.inf, -numpy.inf))\n",
    "\n",
    "    # Initialize parameters\n",
    "    W = init_weights()\n",
    "    momentum = init_momentum()\n",
    "    cost = init_cost()\n",
    "\n",
    "    time_elapsed = 0\n",
    "    for i in range(iterations):\n",
    "        start = time.time()\n",
    "        print('Starting iteration {}'.format(i))\n",
    "\n",
    "        # Calculate the output for the current weights\n",
    "        Y = softmax(W[0], X)\n",
    "\n",
    "        # Calculate the cost function for these outputs\n",
    "        cost.rotate(1)\n",
    "        cost[0] = likelihood_function(W[0], T, Y, 0.05)\n",
    "\n",
    "        # If the cost function has decreased increase the learning rate. Otherwise we moved over the \"valey\", so step\n",
    "        # back to the previous iteration and cut the learning rate in half. Also if the cost difference between the\n",
    "        # last 3 iterations is less than the precision we have converged and we should stop\n",
    "        cost_diff = cost[0] - cost[1]\n",
    "        if (cost_diff >= 0):\n",
    "            if (cost_diff > 0 and cost_diff < precision and (cost[1] - cost[2] < precision)):\n",
    "                break\n",
    "            else:\n",
    "                step = step * 1.05\n",
    "        else:\n",
    "            step = step * 0.5\n",
    "            W.rotate(-1)\n",
    "            cost.rotate(-1)\n",
    "            momentum.rotate(-1)\n",
    "            print('Stepping back: {}'.format(cost[0]))\n",
    "            continue\n",
    "\n",
    "        # Calculate the gradient\n",
    "        gradient = numpy.dot(numpy.transpose(T - Y), X) - (0.05 * W[0])\n",
    "\n",
    "        # Calculate the momentum. Momentum makes changing direction when descenting harder. This makes the algorithm\n",
    "        # bump less on the \"walls\" of a \"valey\" when descenting.\n",
    "        momentum.rotate(1)\n",
    "        momentum[0] = 0.5 * momentum[1] + (step * gradient)\n",
    "\n",
    "        # Adjust the weights according to the momentum calculated\n",
    "        W.rotate(1)\n",
    "        W[0] = W[1] + (step * gradient)\n",
    "\n",
    "        time_elapsed += (time.time() - start)\n",
    "\n",
    "        print('Gradient norm: {}, Likelihood: {}, step: {}'.format(numpy.linalg.norm(gradient, ord=2), cost[0], step))\n",
    "        print('Elapsed: {:.3f}s, Remaining: {:.3f}s'.format(time_elapsed,\n",
    "                                                            (time_elapsed / (i + 1)) * (iterations - i + 1)))\n",
    "\n",
    "        # If we did something numerically wrong, then NaNs will appear. In this case we should stop\n",
    "        if (numpy.isnan(numpy.min(W[0]))):\n",
    "            print('NaNs appear')\n",
    "            break\n",
    "\n",
    "    return W[0]\n",
    "\n",
    "def fuzzy_target(traintarget):\n",
    "    ''' Transform the indicators from 0 and 1 to values close to 0 and 1\n",
    "    This seems to work better for some algorithms'''\n",
    "    for row in range(traintarget.shape[0]):\n",
    "        for col in range(traintarget.shape[1]):\n",
    "            if (traintarget[row, col] == 0):\n",
    "                traintarget[row, col] = 0.0001\n",
    "            else:\n",
    "                traintarget[row, col] = 0.9991\n",
    "    return traintarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we load the MNIST data set and execute the gradient descent. In this notebook we executed 10 iterations as example. We have executed 2000 iterations and achieved 91% accuracy.\n",
    "\n",
    "    Starting iteration 1995\n",
    "    Gradient norm: 105.9549758565084, Likelihood: -19176.325895635244, step: 2.542919593969382e-05\n",
    "    Elapsed: 363.595s, Remaining: 1.093s\n",
    "    Starting iteration 1996\n",
    "    Gradient norm: 105.80861520577044, Likelihood: -19174.624621548784, step: 2.670065573667851e-05\n",
    "    Elapsed: 363.789s, Remaining: 0.911s\n",
    "    Starting iteration 1997\n",
    "    Gradient norm: 105.79296612757267, Likelihood: -19172.839674034956, step: 2.8035688523512437e-05\n",
    "    Elapsed: 363.981s, Remaining: 0.729s\n",
    "    Starting iteration 1998\n",
    "    Gradient norm: 105.71900664024812, Likelihood: -19170.966864003105, step: 2.943747294968806e-05\n",
    "    Elapsed: 364.175s, Remaining: 0.547s\n",
    "    Starting iteration 1999\n",
    "    Gradient norm: 105.6966246769972, Likelihood: -19169.001909664195, step: 3.090934659717246e-05\n",
    "    Elapsed: 364.368s, Remaining: 0.364s\n",
    "    Predicted correct 9135 out of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset from storage\n",
      "Reading file ./mnist/train5.txt\n",
      "Read 5421 rows, 784 columns from ./mnist/train5.txt\n",
      "Reading file ./mnist/train9.txt\n",
      "Read 5949 rows, 784 columns from ./mnist/train9.txt\n",
      "Reading file ./mnist/train7.txt\n",
      "Read 6265 rows, 784 columns from ./mnist/train7.txt\n",
      "Reading file ./mnist/train1.txt\n",
      "Read 6742 rows, 784 columns from ./mnist/train1.txt\n",
      "Reading file ./mnist/train2.txt\n",
      "Read 5958 rows, 784 columns from ./mnist/train2.txt\n",
      "Reading file ./mnist/train8.txt\n",
      "Read 5851 rows, 784 columns from ./mnist/train8.txt\n",
      "Reading file ./mnist/train6.txt\n",
      "Read 5918 rows, 784 columns from ./mnist/train6.txt\n",
      "Reading file ./mnist/train0.txt\n",
      "Read 5923 rows, 784 columns from ./mnist/train0.txt\n",
      "Reading file ./mnist/train3.txt\n",
      "Read 6131 rows, 784 columns from ./mnist/train3.txt\n",
      "Reading file ./mnist/train4.txt\n",
      "Read 5842 rows, 784 columns from ./mnist/train4.txt\n",
      "Reading file ./mnist/test0.txt\n",
      "Read 980 rows, 784 columns from ./mnist/test0.txt\n",
      "Reading file ./mnist/test5.txt\n",
      "Read 892 rows, 784 columns from ./mnist/test5.txt\n",
      "Reading file ./mnist/test3.txt\n",
      "Read 1010 rows, 784 columns from ./mnist/test3.txt\n",
      "Reading file ./mnist/test9.txt\n",
      "Read 1009 rows, 784 columns from ./mnist/test9.txt\n",
      "Reading file ./mnist/test7.txt\n",
      "Read 1028 rows, 784 columns from ./mnist/test7.txt\n",
      "Reading file ./mnist/test1.txt\n",
      "Read 1135 rows, 784 columns from ./mnist/test1.txt\n",
      "Reading file ./mnist/test4.txt\n",
      "Read 982 rows, 784 columns from ./mnist/test4.txt\n",
      "Reading file ./mnist/test2.txt\n",
      "Read 1032 rows, 784 columns from ./mnist/test2.txt\n",
      "Reading file ./mnist/test6.txt\n",
      "Read 958 rows, 784 columns from ./mnist/test6.txt\n",
      "Reading file ./mnist/test8.txt\n",
      "Read 974 rows, 784 columns from ./mnist/test8.txt\n",
      "Finished reading MNIST\n",
      "Starting iteration 0\n",
      "Gradient norm: 107630.03553051759, Likelihood: -859627.5846426857, step: 1.0500000000000001e-05\n",
      "Elapsed: 0.194s, Remaining: 2.130s\n",
      "Starting iteration 1\n",
      "Gradient norm: 71370.77435531012, Likelihood: -724373.3387859963, step: 1.1025000000000002e-05\n",
      "Elapsed: 0.390s, Remaining: 1.950s\n",
      "Starting iteration 2\n",
      "Gradient norm: 59429.17094796775, Likelihood: -630139.6566884641, step: 1.1576250000000003e-05\n",
      "Elapsed: 0.584s, Remaining: 1.752s\n",
      "Starting iteration 3\n",
      "Gradient norm: 46590.046859517126, Likelihood: -552534.7363924853, step: 1.2155062500000004e-05\n",
      "Elapsed: 0.780s, Remaining: 1.559s\n",
      "Starting iteration 4\n",
      "Gradient norm: 34777.50047772562, Likelihood: -492534.2358242957, step: 1.2762815625000004e-05\n",
      "Elapsed: 0.974s, Remaining: 1.363s\n",
      "Starting iteration 5\n",
      "Gradient norm: 31106.65420963571, Likelihood: -446250.1142562222, step: 1.3400956406250006e-05\n",
      "Elapsed: 1.171s, Remaining: 1.171s\n",
      "Starting iteration 6\n",
      "Gradient norm: 28525.32367856063, Likelihood: -406217.2387419829, step: 1.4071004226562506e-05\n",
      "Elapsed: 1.366s, Remaining: 0.976s\n",
      "Starting iteration 7\n",
      "Gradient norm: 26175.336962689842, Likelihood: -370647.2774024288, step: 1.4774554437890633e-05\n",
      "Elapsed: 1.563s, Remaining: 0.782s\n",
      "Starting iteration 8\n",
      "Gradient norm: 24089.111269807432, Likelihood: -339059.75578989624, step: 1.5513282159785166e-05\n",
      "Elapsed: 1.759s, Remaining: 0.586s\n",
      "Starting iteration 9\n",
      "Gradient norm: 22242.901414861877, Likelihood: -310974.6211984769, step: 1.6288946267774425e-05\n",
      "Elapsed: 1.956s, Remaining: 0.391s\n",
      "Predicted correct 3746 out of 10000\n"
     ]
    }
   ],
   "source": [
    "traindata, traintarget, testdata, testtarget = load_mnist('./mnist')\n",
    "#traindata, traintarget, testdata, testtarget = unpickle_mnist()\n",
    "\n",
    "traintarget = fuzzy_target(traintarget)\n",
    "\n",
    "W = gradient_descent(traindata / 255, traintarget, 0.00001, 10, 0.00001) # 10 Iterations\n",
    "Y = softmax(W, testdata / 255)\n",
    "res = numpy.argmax(Y, axis=1) - numpy.argmax(testtarget, axis=1)\n",
    "zeros = 0\n",
    "for row in range(res.shape[0]):\n",
    "    if (res[row] == 0):\n",
    "        zeros = zeros + 1\n",
    "print('Predicted correct {} out of 10000'.format(zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we load the CIFAR10 dataset and execute the gradient descent. In this notebook we executed 10 iterations. We have also executed 100,000 iteration and achieved 34% accuracy:\n",
    "\n",
    "    Starting iteration 99995\n",
    "    Gradient norm: 340.2301905652409, Likelihood: -93859.43385195508, step: 9.213183965312024e-07\n",
    "    Elapsed: 70281.315s, Remaining: 4.217s\n",
    "    Starting iteration 99996\n",
    "    Gradient norm: 654.7906132198642, Likelihood: -93859.23399771262, step: 9.673843163577625e-07\n",
    "    Elapsed: 70282.091s, Remaining: 3.514s\n",
    "    Starting iteration 99997\n",
    "    Gradient norm: 1472.3313849536962, Likelihood: -93859.23071787521, step: 1.0157535321756506e-06\n",
    "    Elapsed: 70282.866s, Remaining: 2.811s\n",
    "    Starting iteration 99998\n",
    "    Stepping back: -93859.23071787521\n",
    "    Starting iteration 99999\n",
    "    Gradient norm: 1472.3313849536962, Likelihood: -93859.23071787521, step: 5.332706043922166e-07\n",
    "    Elapsed: 70283.399s, Remaining: 1.406s\n",
    "    Predicted correct 3391 out of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CIFAR-10 dataset from storage\n",
      "Starting iteration 0\n",
      "Gradient norm: 1014474.511582108, Likelihood: -1296218.8497923529, step: 1.0500000000000001e-05\n",
      "Elapsed: 0.590s, Remaining: 6.491s\n",
      "Starting iteration 1\n",
      "Gradient norm: 396788.47955721786, Likelihood: -1044234.4758379774, step: 1.1025000000000002e-05\n",
      "Elapsed: 1.373s, Remaining: 6.866s\n",
      "Starting iteration 2\n",
      "Stepping back: -1044234.4758379774\n",
      "Starting iteration 3\n",
      "Gradient norm: 396788.47955721786, Likelihood: -1044234.4758379774, step: 5.7881250000000014e-06\n",
      "Elapsed: 1.959s, Remaining: 3.918s\n",
      "Starting iteration 4\n",
      "Stepping back: -1044234.4758379774\n",
      "Starting iteration 5\n",
      "Gradient norm: 396788.47955721786, Likelihood: -1044234.4758379774, step: 3.038765625000001e-06\n",
      "Elapsed: 2.577s, Remaining: 2.577s\n",
      "Starting iteration 6\n",
      "Gradient norm: 333578.87065147684, Likelihood: -1013476.6112241243, step: 3.190703906250001e-06\n",
      "Elapsed: 3.303s, Remaining: 2.359s\n",
      "Starting iteration 7\n",
      "Gradient norm: 249901.01778345762, Likelihood: -971541.7241861853, step: 3.3502391015625014e-06\n",
      "Elapsed: 4.041s, Remaining: 2.021s\n",
      "Starting iteration 8\n",
      "Gradient norm: 198887.11529002295, Likelihood: -929292.9540926282, step: 3.5177510566406266e-06\n",
      "Elapsed: 4.777s, Remaining: 1.592s\n",
      "Starting iteration 9\n",
      "Gradient norm: 221788.16038956103, Likelihood: -919348.2522061034, step: 3.6936386094726582e-06\n",
      "Elapsed: 5.516s, Remaining: 1.103s\n",
      "Predicted correct 1105 out of 10000\n"
     ]
    }
   ],
   "source": [
    "traindata, traintarget, testdata, testtarget = load_cifar('./cifar-10-batches-py')\n",
    "\n",
    "traintarget = fuzzy_target(traintarget)\n",
    "\n",
    "W = gradient_descent(traindata / 255, traintarget, 0.00001, 10, 0.00001) # 10 Iterations\n",
    "Y = softmax(W, testdata / 255)\n",
    "res = numpy.argmax(Y, axis=1) - numpy.argmax(testtarget, axis=1)\n",
    "zeros = 0\n",
    "for row in range(res.shape[0]):\n",
    "    if (res[row] == 0):\n",
    "        zeros = zeros + 1\n",
    "print('Predicted correct {} out of 10000'.format(zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files gradient_descent.py, mnist.py and cifar10.py include the above code that can be executed on the shell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
