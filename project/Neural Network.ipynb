{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charalampos Kaidos & Ioannis Papantonis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same directory as this notebook, create a folder named \"mnist\" and put inside the mnist data that where shipped with the project anouncement. In a separate folder called \"cifar-10-batches-py\" put the cifar10 pickled dataset as provided for python in the CIFAR website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow load MNIST data set from the files provided in the excercise. Also there are functions to save the data in pickled format and load them from pickled format which is much faster than loading them from the texr files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "def load_mnist(path):\n",
    "    print('Reading MNIST dataset from storage')\n",
    "\n",
    "    trainfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and 'train' in f]\n",
    "    train_data, train_target = load_mnist_from_files(trainfiles)\n",
    "\n",
    "    testfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and 'test' in f]\n",
    "    test_data, test_target = load_mnist_from_files(testfiles)\n",
    "\n",
    "    print('Finished reading MNIST')\n",
    "    return train_data, train_target, test_data, test_target\n",
    "\n",
    "def load_mnist_from_files(files):\n",
    "    data = None\n",
    "    target = None\n",
    "\n",
    "    for file in files:\n",
    "        print('Reading file {}'.format(file))\n",
    "        temp_data = numpy.loadtxt(file)\n",
    "        if (numpy.any(data) == None):\n",
    "            data = temp_data\n",
    "        else:\n",
    "            data = numpy.append(data, temp_data, axis=0)\n",
    "\n",
    "        rows, columns = temp_data.shape\n",
    "        temp_target = numpy.zeros((rows, 10))\n",
    "        temp_target[:, int(file[-5])] = numpy.ones(rows)\n",
    "        if (numpy.any(target) == None):\n",
    "            target = temp_target\n",
    "        else:\n",
    "            target = numpy.append(target, temp_target, axis=0)\n",
    "\n",
    "        print('Read {} rows, {} columns from {}'.format(rows, columns, file))\n",
    "\n",
    "    return data, target\n",
    "\n",
    "def pickle_mnist(traindata, traintarget, testdata, testtarget):\n",
    "    f=open('./mnist/trdata', mode='wb+')\n",
    "    pickle.dump(traindata, f)\n",
    "    f.close()\n",
    "    f = open('./mnist/trtarget', mode='wb+')\n",
    "    pickle.dump(traintarget, f)\n",
    "    f.close()\n",
    "    f = open('./mnist/tedata', mode='wb+')\n",
    "    pickle.dump(testdata, f)\n",
    "    f.close\n",
    "    f = open('./mnist/tetarget', mode='wb+')\n",
    "    pickle.dump(testtarget, f)\n",
    "    f.close()\n",
    "\n",
    "def unpickle_mnist():\n",
    "    fo = open('./mnist/trdata', 'rb')\n",
    "    traindata = pickle.load(fo)\n",
    "    fo.close()\n",
    "    fo = open('./mnist/trtarget', 'rb')\n",
    "    traintarget = pickle.load(fo)\n",
    "    fo.close()\n",
    "    fo = open('./mnist/tedata', 'rb')\n",
    "    testdata = pickle.load(fo)\n",
    "    fo.close()\n",
    "    fo = open('./mnist/tetarget', 'rb')\n",
    "    testtarget = pickle.load(fo)\n",
    "    fo.close()\n",
    "    return traindata, traintarget, testdata, testtarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow load the CIFAR-10 data set from the pickled format provided on the CIFAR website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "def load_cifar(path):\n",
    "    print('Reading CIFAR-10 dataset from storage')\n",
    "\n",
    "    dicts = []\n",
    "\n",
    "    trainfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and 'data_batch' in f]\n",
    "    for f in trainfiles:\n",
    "        dicts.append(unpickle(f))\n",
    "\n",
    "    data = numpy.vstack(tuple([x['data'] for x in dicts]))\n",
    "    ones = numpy.ones((data.shape[0], 1))\n",
    "    data = numpy.hstack((ones,data))\n",
    "\n",
    "    temp_target = numpy.hstack(tuple([x['labels'] for x in dicts]))\n",
    "    target = numpy.zeros((temp_target.shape[0], 10))\n",
    "    for r in range(temp_target.shape[0]):\n",
    "        target[r, temp_target[r]] = 1\n",
    "\n",
    "    test_data = unpickle(join(path, 'test_batch'))\n",
    "    test_target = numpy.zeros((len(test_data['labels']), 10))\n",
    "    for r in range(test_target.shape[0]):\n",
    "        test_target[r, test_data['labels'][r]] = 1\n",
    "    test_data = test_data['data']\n",
    "    ones = numpy.ones((test_data.shape[0], 1))\n",
    "    test_data = numpy.hstack((ones, test_data))\n",
    "\n",
    "    return data, target, test_data, test_target\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dictionary = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow implement the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, struct\n",
    "import time\n",
    "\n",
    "def indicators(t):     # returns array with indicator vectors as rows\n",
    "    (N,)=t.shape\n",
    "    if np.min(t)>0:\n",
    "        categories=np.max(t) \n",
    "        ind=np.zeros((N,categories))\n",
    "        for n in range(0,N):\n",
    "            ind[n,t[n]-1]=1\n",
    "    else:\n",
    "        categories=np.max(t)+1 \n",
    "        ind=np.zeros((N,categories))\n",
    "        for n in range(0,N):\n",
    "            ind[n,t[n]]=1\n",
    "    return ind\n",
    "\n",
    "def softmax(A):  # definition of softmax function, operating on each row of matrix A\n",
    "    A = A - np.amax(A, axis = 1).reshape(A.shape[0],1)\n",
    "    A = np.exp(A)\n",
    "    A = A / np.sum(A, axis=1).reshape(A.shape[0],1)\n",
    "    return A\n",
    "\n",
    "def sigmoid(A):  # definition of the sigmoid function \n",
    "    return 1 / (np.exp(-A) + 1)\n",
    "\n",
    "def activation(A):   # returns the outputs of the hidden layer\n",
    "    return np.log(1 + np.exp(A))\n",
    "\n",
    "def output(X, W2):  # returns the activations of the output neurons\n",
    "    ones = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((ones,X))\n",
    "    A = np.dot(X, W2)\n",
    "    return softmax(A)\n",
    "\n",
    "def middle_deltas(X,W,out_deltas):  # helper function that calculates the deltas regarding the hidden layer\n",
    "    [W1, W2] = W\n",
    "    A = np.dot(X, W1)\n",
    "    no_bias = np.delete(W2, 0, axis=0)  # we discard the bias parameters\n",
    "    deriv_act = sigmoid(A)   #  the derivative of the hidden layer function, is the sigmoid \n",
    "    sums = np.dot(out_deltas, no_bias.T)  \n",
    "    return sums * deriv_act\n",
    "\n",
    "def middle_grad(X,W,out_deltas):  # returns the gradient of the parameters connecting the input with the hidden layer\n",
    "    d2 = middle_deltas(X, W, out_deltas)\n",
    "    grad = np.dot(X.T, d2)\n",
    "    return grad\n",
    "\n",
    "def forward_pass(X,W):  #  returns the outputs of the neural network\n",
    "    [W1,W2]=W\n",
    "    act=activation(np.dot(X, W1))\n",
    "    out=output(act,W2)\n",
    "    return out\n",
    "\n",
    "def activations_outputs(X,W): #the same as above, but also returns the activations\n",
    "    [W1,W2] = W\n",
    "    act=activation(np.dot(X,W1))\n",
    "    out=output(act,W2)\n",
    "    return [act,out]\n",
    "\n",
    "def gradient(X,T,W):  # returns a list with the gradient of the hidden layer parameters as first element, and the output's layer as second\n",
    "    [act, out] = activations_outputs(X, W)\n",
    "    D1 = T-out\n",
    "    mid_grad = middle_grad(X, W, D1)\n",
    "    ones = np.ones((act.shape[0],1))\n",
    "    act = np.hstack((ones, act))  # for the bias\n",
    "    out_grad = np.dot(act.T, D1)\n",
    "    return [mid_grad, out_grad]\n",
    "\n",
    "def cost(X,T,W,l):   #the cost function\n",
    "    Y = forward_pass(X, W)\n",
    "    [W1, W2] = W\n",
    "    return np.sum(T * np.log(Y + np.finfo(float).eps)) - l / 2 * (np.sum(np.linalg.norm(W1, axis=0) ** 2) + np.sum(np.linalg.norm(W2, axis=0) ** 2))\n",
    "\n",
    "def computed_gradient(X,T,W,l):  # gradient check\n",
    "    [w1,w2]=W\n",
    "    (M,D)=w1.shape\n",
    "    (K,N)=w2.shape\n",
    "    gr1=np.zeros(w1.shape)\n",
    "    gr2=np.zeros(w2.shape)\n",
    "    for m in range(0,M):\n",
    "        for d in range(0,D):\n",
    "            plus=np.copy(w1)\n",
    "            minus=np.copy(w1)\n",
    "            plus[m][d]=plus[m][d]+1e-06\n",
    "            minus[m][d]=minus[m][d]-1e-06\n",
    "            gr1[m,d]=(cost(X,T,[plus,w2],l)-cost(X,T,[minus,w2],l))/2*1e-06\n",
    "    for k in range(0,k):\n",
    "        for n in range(0,N):\n",
    "            plus=np.copy(w2)\n",
    "            minus=np.copy(w2)\n",
    "            plus[k][n]=plus[k][n]+1e-06\n",
    "            minus[k][n]=minus[k][n]-1e-06\n",
    "            gr2[k,n]=(cost(X,T,[w1,plus],l)-cost(X,T,[w1,minus],l))/2*1e-06\n",
    "    return [gr1,gr2]\n",
    "\n",
    "def train(X,T,init,l,etta,iterations): #X array (N,D+1), T array (N,K), init is the initial guess for the weights, l regularization parameter\n",
    "    (N,D) = X.shape                    # etta is the learning rate, iterations is the maximum number of iterations\n",
    "    E_old=-np.inf\n",
    "    E_new=cost(X,T,init,l)\n",
    "    W=init\n",
    "    [W1, W2] = W\n",
    "    i = 0\n",
    "    time_elapsed = 0\n",
    "    while(np.abs(E_new-E_old)>0.001):\n",
    "        start = time.time()\n",
    "        print('Starting iteration {}'.format(i))\n",
    "        E_old=E_new\n",
    "        (grad1, grad2) = gradient(X, T, W)\n",
    "        #t = computed_gradient(X,T,W,l)\n",
    "        grad1=grad1-l*W1   \n",
    "        grad2=grad2-l*W2\n",
    "        W1=W1+etta*grad1   # gradient ascent update\n",
    "        W2=W2+etta*grad2   # gradient ascent update\n",
    "        W=[W1,W2]\n",
    "        E_new=cost(X,T,W,l)\n",
    "        print(E_new)\n",
    "        time_elapsed += (time.time() - start)\n",
    "        print('Elapsed: {:.3f}s, Remaining: {:.3f}s'.format(time_elapsed,(time_elapsed / (i + 1)) * (iterations - i + 1)))\n",
    "        i += 1\n",
    "        if(i > iterations):\n",
    "            break\n",
    "    return W\n",
    "\n",
    "def predict(X,W):   #X must have 1s as first column, returns predicted labels\n",
    "    (N,D)=X.shape\n",
    "    pred=np.zeros(N)\n",
    "    Y = forward_pass(X, W)\n",
    "    pred = np.argmax(Y, axis=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we train a neural network on the MNIST data set. We have executed 10 iterations here. On 3000 iterations we got accuracy 87%. Here we have disabled gradient check for performance reasons.\n",
    "\n",
    "    Starting iteration 2995\n",
    "    -38232.0481124\n",
    "    Elapsed: 2781.969s, Remaining: 5.571s\n",
    "    Starting iteration 2996\n",
    "    -38177.6512299\n",
    "    Elapsed: 2782.919s, Remaining: 4.643s\n",
    "    Starting iteration 2997\n",
    "    -38224.8117385\n",
    "    Elapsed: 2783.854s, Remaining: 3.714s\n",
    "    Starting iteration 2998\n",
    "    -38170.4619134\n",
    "    Elapsed: 2784.780s, Remaining: 2.786s\n",
    "    Starting iteration 2999\n",
    "    -38217.5836823\n",
    "    Elapsed: 2785.715s, Remaining: 1.857s\n",
    "    Starting iteration 3000\n",
    "    -38163.2808416\n",
    "    Elapsed: 2786.652s, Remaining: 0.929s\n",
    "    Predicted correct 8680 out of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset from storage\n",
      "Reading file ./mnist/train5.txt\n",
      "Read 5421 rows, 784 columns from ./mnist/train5.txt\n",
      "Reading file ./mnist/train9.txt\n",
      "Read 5949 rows, 784 columns from ./mnist/train9.txt\n",
      "Reading file ./mnist/train7.txt\n",
      "Read 6265 rows, 784 columns from ./mnist/train7.txt\n",
      "Reading file ./mnist/train1.txt\n",
      "Read 6742 rows, 784 columns from ./mnist/train1.txt\n",
      "Reading file ./mnist/train2.txt\n",
      "Read 5958 rows, 784 columns from ./mnist/train2.txt\n",
      "Reading file ./mnist/train8.txt\n",
      "Read 5851 rows, 784 columns from ./mnist/train8.txt\n",
      "Reading file ./mnist/train6.txt\n",
      "Read 5918 rows, 784 columns from ./mnist/train6.txt\n",
      "Reading file ./mnist/train0.txt\n",
      "Read 5923 rows, 784 columns from ./mnist/train0.txt\n",
      "Reading file ./mnist/train3.txt\n",
      "Read 6131 rows, 784 columns from ./mnist/train3.txt\n",
      "Reading file ./mnist/train4.txt\n",
      "Read 5842 rows, 784 columns from ./mnist/train4.txt\n",
      "Reading file ./mnist/test0.txt\n",
      "Read 980 rows, 784 columns from ./mnist/test0.txt\n",
      "Reading file ./mnist/test5.txt\n",
      "Read 892 rows, 784 columns from ./mnist/test5.txt\n",
      "Reading file ./mnist/test3.txt\n",
      "Read 1010 rows, 784 columns from ./mnist/test3.txt\n",
      "Reading file ./mnist/test9.txt\n",
      "Read 1009 rows, 784 columns from ./mnist/test9.txt\n",
      "Reading file ./mnist/test7.txt\n",
      "Read 1028 rows, 784 columns from ./mnist/test7.txt\n",
      "Reading file ./mnist/test1.txt\n",
      "Read 1135 rows, 784 columns from ./mnist/test1.txt\n",
      "Reading file ./mnist/test4.txt\n",
      "Read 982 rows, 784 columns from ./mnist/test4.txt\n",
      "Reading file ./mnist/test2.txt\n",
      "Read 1032 rows, 784 columns from ./mnist/test2.txt\n",
      "Reading file ./mnist/test6.txt\n",
      "Read 958 rows, 784 columns from ./mnist/test6.txt\n",
      "Reading file ./mnist/test8.txt\n",
      "Read 974 rows, 784 columns from ./mnist/test8.txt\n",
      "Finished reading MNIST\n",
      "Starting iteration 0\n",
      "-1697378.94431\n",
      "Elapsed: 1.067s, Remaining: 11.732s\n",
      "Starting iteration 1\n",
      "-1589429.96229\n",
      "Elapsed: 2.063s, Remaining: 10.314s\n",
      "Starting iteration 2\n",
      "-1500870.19741\n",
      "Elapsed: 3.047s, Remaining: 9.140s\n",
      "Starting iteration 3\n",
      "-1426145.07635\n",
      "Elapsed: 4.000s, Remaining: 8.000s\n",
      "Starting iteration 4\n",
      "-1354568.48379\n",
      "Elapsed: 4.969s, Remaining: 6.957s\n",
      "Starting iteration 5\n",
      "-1285239.46517\n",
      "Elapsed: 5.950s, Remaining: 5.950s\n",
      "Starting iteration 6\n",
      "-1218533.30145\n",
      "Elapsed: 6.878s, Remaining: 4.913s\n",
      "Starting iteration 7\n",
      "-1155751.25847\n",
      "Elapsed: 7.842s, Remaining: 3.921s\n",
      "Starting iteration 8\n",
      "-1097290.70746\n",
      "Elapsed: 8.790s, Remaining: 2.930s\n",
      "Starting iteration 9\n",
      "-1043092.05961\n",
      "Elapsed: 9.758s, Remaining: 1.952s\n",
      "Starting iteration 10\n",
      "-993063.215084\n",
      "Elapsed: 10.697s, Remaining: 0.972s\n",
      "Predicted correct 2572 out of 10000\n"
     ]
    }
   ],
   "source": [
    "traindata, traintarget, testdata, testtarget = load_mnist('./mnist')\n",
    "\n",
    "def init_weights():  # initial guess for the weights\n",
    "    M = 50\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(traindata.shape[1], M)\n",
    "    W2 = np.random.randn(M+1, traintarget.shape[1])\n",
    "    W = [W1, W2]\n",
    "    return W\n",
    "\n",
    "W = train(traindata/255, traintarget, init_weights(), 0.0001, 0.000001,10)\n",
    "Y = predict(testdata/255, W)\n",
    "res = Y - np.argmax(testtarget, axis=1)\n",
    "zeros = 0\n",
    "for row in range(res.shape[0]):\n",
    "    if(res[row] == 0):\n",
    "        zeros = zeros +1\n",
    "print('Predicted correct {} out of 10000'.format(zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we train a neural network on the CIFAR data set. We have executed 10 iterations here. On 3000 iterations we got accuracy 30%. Here we have disabled gradient check for performance reasons.\n",
    "\n",
    "    Starting iteration 2997\n",
    "    -116214.946819\n",
    "    Elapsed: 6168.088s, Remaining: 8.230s\n",
    "    Starting iteration 2998\n",
    "    -116214.507111\n",
    "    Elapsed: 6170.337s, Remaining: 6.172s\n",
    "    Starting iteration 2999\n",
    "    -116214.067669\n",
    "    Elapsed: 6172.802s, Remaining: 4.115s\n",
    "    Starting iteration 3000\n",
    "    -116213.62849\n",
    "    Elapsed: 6175.156s, Remaining: 2.058s\n",
    "    Predicted correct 3026 out of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CIFAR-10 dataset from storage\n",
      "Starting iteration 0\n",
      "-1540563.48165\n",
      "Elapsed: 2.301s, Remaining: 25.308s\n",
      "Starting iteration 1\n",
      "-1494516.60519\n",
      "Elapsed: 4.468s, Remaining: 22.338s\n",
      "Starting iteration 2\n",
      "-1469674.16252\n",
      "Elapsed: 6.455s, Remaining: 19.366s\n",
      "Starting iteration 3\n",
      "-1351975.72548\n",
      "Elapsed: 8.536s, Remaining: 17.072s\n",
      "Starting iteration 4\n",
      "-1171866.99918\n",
      "Elapsed: 10.517s, Remaining: 14.724s\n",
      "Starting iteration 5\n",
      "-747319.826937\n",
      "Elapsed: 12.620s, Remaining: 12.620s\n",
      "Starting iteration 6\n",
      "-637967.095752\n",
      "Elapsed: 14.593s, Remaining: 10.424s\n",
      "Starting iteration 7\n",
      "-560894.989355\n",
      "Elapsed: 16.567s, Remaining: 8.284s\n",
      "Starting iteration 8\n",
      "-503081.853704\n",
      "Elapsed: 18.561s, Remaining: 6.187s\n",
      "Starting iteration 9\n",
      "-458231.340626\n",
      "Elapsed: 20.620s, Remaining: 4.124s\n",
      "Starting iteration 10\n",
      "-422539.193236\n",
      "Elapsed: 22.599s, Remaining: 2.054s\n",
      "Predicted correct 1176 out of 10000\n"
     ]
    }
   ],
   "source": [
    "traindata, traintarget, testdata, testtarget = load_cifar('./cifar-10-batches-py')\n",
    "\n",
    "def init_weights():  # initial guess for the weights\n",
    "    M = 50\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(traindata.shape[1], M)\n",
    "    W2 = np.random.randn(M+1, traintarget.shape[1])\n",
    "    W = [W1, W2]\n",
    "    return W\n",
    "\n",
    "W = train(traindata/255, traintarget, init_weights(), 0.0001, 0.000001,10)\n",
    "Y = predict(testdata/255, W)\n",
    "res = Y - np.argmax(testtarget, axis=1)\n",
    "zeros = 0\n",
    "for row in range(res.shape[0]):\n",
    "    if(res[row] == 0):\n",
    "        zeros = zeros +1\n",
    "print('Predicted correct {} out of 10000'.format(zeros))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
